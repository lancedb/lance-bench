name: Run Python Benchmarks

on:
  workflow_call:
    inputs:
      git_sha:
        description: "Git SHA to benchmark"
        required: true
        type: string
      pr_number:
        description: "PR number (enables PR mode - local results only)"
        required: false
        type: number
    secrets:
      LANCE_BENCH_DB_URI:
        required: true
      BENCH_S3_USER_ACCESS_KEY:
        required: true
      BENCH_S3_USER_SECRET_KEY:
        required: true
  workflow_dispatch:
    inputs:
      git_sha:
        description: "Git SHA to benchmark"
        required: true
        type: string

jobs:
  build:
    uses: ./.github/workflows/build-lance-python.yml
    with:
      git_sha: ${{ inputs.git_sha }}

  benchmark:
    needs: build
    runs-on: warp-ubuntu-latest-x64-8x
    steps:
      - name: Checkout lance-bench repository
        uses: actions/checkout@v4

      - name: Checkout lance repository
        uses: actions/checkout@v4
        with:
          repository: lance-format/lance
          ref: ${{ inputs.git_sha }}
          path: lance

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Download Lance wheel
        uses: actions/download-artifact@v4
        with:
          name: lance-wheel
          path: wheels/

      - name: Create virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install wheels/*.whl pytest pytest-benchmark requests duckdb

      - name: Generate benchmark datasets
        run: |
          source venv/bin/activate
          export PYTHONPATH="${PYTHONPATH}:${PWD}/lance/python/python"
          cd lance/python/python
          python -m ci_benchmarks.datagen.gen_all

      - name: Run Python benchmarks
        run: |
          source venv/bin/activate
          export PYTHONPATH="${PYTHONPATH}:${PWD}/lance/python/python"
          cd lance/python/python
          pytest ci_benchmarks/benchmarks \
            --benchmark-json=../../../pytest-output.json \
            --benchmark-only

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install lance-bench dependencies
        run: uv sync

      - name: Publish results
        env:
          LANCE_BENCH_URI: ${{ inputs.pr_number && './pr-results.lance' || secrets.LANCE_BENCH_DB_URI }}
          AWS_ACCESS_KEY_ID: ${{ secrets.BENCH_S3_USER_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BENCH_S3_USER_SECRET_KEY }}
        run: |
          uv run python scripts/publish_pytest.py \
            pytest-output.json \
            --testbed-name "ubuntu-latest" \
            --dut-name "lance" \
            --dut-version "${{ needs.build.outputs.dut_version }}" \
            --dut-timestamp ${{ needs.build.outputs.timestamp }}

      - name: Upload results
        if: inputs.pr_number
        uses: actions/upload-artifact@v4
        with:
          name: pr-results-python
          path: ./pr-results.lance
